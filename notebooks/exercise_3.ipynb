{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downtime Challenge | Exercise 3\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils.exercise_3 import all_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"../data/dbs/Ex3.db\")\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In the last exercise, we looked at incidents spanning multiple tables in a database. Yet, we've still only been looking at _individual_ metrics like the row count, rate of null values, and so on. In practice, many genuine data downtime incidents involve _conjunctions_ of events across multiple upstream and downstream tables. In this exercise, we practice crafting single queries that can handle such conjunctive events.\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "In this exercise, we'll continue to use the `EXOPLANETS`, `HABITABLES`, and `EXOPLANETS_SCHEMA` tables. There is also a `HABITABLES_SCHEMA` table, and all four contain additional entries beyond those in exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all tables in DB\n",
    "pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        name\n",
    "    FROM \n",
    "        sqlite_master \n",
    "    WHERE \n",
    "        type ='table' AND \n",
    "        name NOT LIKE 'sqlite_%';\n",
    "    \"\"\",\n",
    "    conn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: Pillars of Data Observability in Conjunction\n",
    "\n",
    "Why care about conjunctions of events, when individual events provide all the information you need? One important factor is **noise** -- looking at simultaneous events reduces the total number of events you're worried about, and makes it more lilely that the issues are genuine. Another factor is **causality** -- given an issue in some table, looking at simultaneous events in upstream tables might help you determine the root cause, and quicken the path to a solution.\n",
    "\n",
    "Take the past exercise as an example -- we saw that the `habitability` field had anomalous rates of zeroed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero = pd.read_sql_query(\"\"\"\n",
    "SELECT\n",
    "    DATE_ADDED,\n",
    "    CAST(SUM(CASE WHEN HABITABILITY IS 0 THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS ZERO_RATE\n",
    "FROM\n",
    "    HABITABLES\n",
    "GROUP BY\n",
    "    DATE_ADDED\n",
    "\"\"\", conn)\n",
    "\n",
    "h_zero = h_zero \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in h_zero.columns}) \\\n",
    "    .set_index(\"date_added\") \\\n",
    "    .reindex(all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero_alerts = pd.read_sql_query(\"\"\"\n",
    "WITH HABITABILITY_ZERO_RATE AS(\n",
    "SELECT\n",
    "    DATE_ADDED,\n",
    "    CAST(SUM(CASE WHEN HABITABILITY IS 0 THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS ZERO_RATE\n",
    "FROM\n",
    "    HABITABLES\n",
    "GROUP BY\n",
    "    DATE_ADDED\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    DATE_ADDED\n",
    "FROM\n",
    "    HABITABILITY_ZERO_RATE\n",
    "WHERE\n",
    "    ZERO_RATE IS NOT NULL AND\n",
    "    ZERO_RATE > 0.3\n",
    "\"\"\", conn)\n",
    "h_zero_alerts = h_zero_alerts \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in h_zero_alerts.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=h_zero[\"zero_rate\"])\n",
    "for alert in h_zero_alerts[\"date_added\"]: plt.axvline(x = alert, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero_alerts[\"date_added\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly identifies some problematic timestamps, but it's much too noisy. We wouldn't want a notification for every red line on the above graph. Looking for other (upstream) events can not only prune our alerts, but also help us identify the issue's cause.\n",
    "\n",
    "See if you can join the timestamps from `h_zero_alerts` with timestamps identifying a `schema_change`.\n",
    "\n",
    "- *Hint*: try querying the `EXOPLANETS_SCHEMA` table using the same approach from Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_anoms = pd.read_sql_query(SQL, conn)\n",
    "joint_anoms = joint_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in joint_anoms.columns})\n",
    "joint_anoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=h_zero[\"zero_rate\"])\n",
    "for alert in joint_anoms[\"date\"]: plt.axvline(x = alert, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a single reported date, `2020-07-19`. Not only have we reduced the number of reports, but we potentially learn something -- that the schema change in `EXOPLANETS` _caused_ the zero rate in `HABITABLES` to spike. By combining data observability pillars, we're one step closer to resolving the problem!\n",
    "\n",
    "![SegmentLocal](/tree/data/assets/comet.gif \"segment\")\n",
    "\n",
    "## 4. Exercise: Diagnosing Another Distribution Issue\n",
    "\n",
    "Here's another quick mystery. It looks like the `HABITABLES` table returns to normal after a while, if we only look at zero rates. But probing into the **volume** of the table reveals something odd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_added = pd.read_sql_query(\"\"\"\n",
    "SELECT\n",
    "    DATE_ADDED,\n",
    "    COUNT(*) AS ROWS_ADDED\n",
    "FROM\n",
    "    HABITABLES\n",
    "GROUP BY\n",
    "    DATE_ADDED\n",
    "\"\"\", conn)\n",
    "rows_added = rows_added \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in rows_added.columns}) \\\n",
    "    .set_index(\"date_added\") \\\n",
    "    .reindex(all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=rows_added[\"rows_added\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row counts added seem to increase by ~1.5x each day starting around `DATE`. We could detect this using a naive threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_rc_anoms = pd.read_sql_query(\"\"\"\n",
    "WITH ROW_COUNTS AS(\n",
    "    SELECT\n",
    "        DATE_ADDED,\n",
    "        COUNT(*) AS ROWS_ADDED\n",
    "    FROM\n",
    "        HABITABLES\n",
    "    GROUP BY\n",
    "        DATE_ADDED\n",
    ")\n",
    "SELECT\n",
    "    DATE_ADDED\n",
    "FROM\n",
    "    ROW_COUNTS\n",
    "WHERE\n",
    "    ROWS_ADDED > 150\n",
    "\"\"\", conn)\n",
    "h_rc_anoms = h_rc_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in h_rc_anoms.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=rows_added[\"rows_added\"])\n",
    "for alert in h_rc_anoms[\"date_added\"]: plt.axvline(x = alert, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again, that's not too noisy to be informative. This issue is compounded because volume is usually a problem when it *decreases* (as we saw earlier with freshness). But something must be the cause of this volume change, and turns out, it's a genuine issue.\n",
    "\n",
    "As another exercise in understanding **distribution**, try querying for _uniqueness_ in `HABITABLES` to find a simultaneous issue.\n",
    "\n",
    "- *Hint*: What's practically guaranteed to be unique in this table, if anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_uniq = pd.read_sql_query(SQL, conn)\n",
    "h_uniq = h_uniq.rename(columns={clmn: clmn.lower() for clmn in h_uniq.columns}) \\\n",
    "    .set_index(\"date_added\") \\\n",
    "    .reindex(all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=h_uniq[\"pct_unique\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proper query here should reveal something telling -- the `_ID` field in `HABITABLES` is not unique, meaning we may be adding duplicate entries to our table! Semantics should dictate that `_ID` be 100% unique. Try writing a query that turns up the offending dates below:\n",
    "- As an extra challenge, try filtering all alerts that occur immediately after other ones -- another potential technique for reducing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_uniq_anoms = pd.read_sql_query(SQL, conn)\n",
    "h_uniq_anoms = h_uniq_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in h_uniq_anoms.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_uniq_anoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(all_days, height=h_uniq[\"pct_unique\"])\n",
    "for alert in h_uniq_anoms[\"date_added\"]: plt.axvline(x = alert, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great work!\n",
    "\n",
    "This last exercise revealed that certain pillars of data observability are often conjoined to give meaningful alerts (volume and uniqueness; schema change and downstream distributions; etc.). In the next exercise, we'll look at some terms from machine learning to scale our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
